import streamlit as st
import cv2 as cv
from PIL import Image, ImageOps
from tensorflow.keras import layers, models
import pickle
import numpy as np
from streamlit_drawable_canvas import st_canvas

st.set_page_config(
    page_title="üéàHoang Hao's Applications",
    page_icon=Image.open("./images/Logo/logo_welcome.png"),
    layout="wide",
    initial_sidebar_state="expanded",
)
st.title("Handwriting Letter Recognize")

def predict_with_image(image):
    model = models.Sequential()

    # Convolutional Block 1
    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)))
    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2, 2)))

    # Convolutional Block 2
    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2, 2)))

    # Flatten the feature maps
    model.add(layers.Flatten())

    # Fully Connected Block 1
    model.add(layers.Dense(128, activation='relu'))

    # Output Layer
    model.add(layers.Dense(10, activation='softmax')) 

    loaded_weights = []
    with open("./data_processed/Handwriting_Letter_Recognize/weight.pkl", "rb") as file:
        loaded_weights = pickle.load(file)

    model.set_weights(loaded_weights)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Process image
    if len(image.shape) == 3: 
        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)
    image_cpy = image.copy()
    image[image_cpy != 0] = 255
    # st.image(image)
    image = cv.resize(image, (28, 28))
    image = image = image.astype('float32') / 255.0
    # Reshape ((batch_size, height, width, channels))
    image = image.reshape((1, 28, 28, 1))
    predictions = model.predict(image)
    predicted_labels = np.argmax(predictions, axis=1)
    return predicted_labels

def crop_and_center_mnist(img):
    img_cpy = img.copy()
    id_white = np.where(img_cpy > 0)

    x_min = np.min(id_white[1])  
    x_max = np.max(id_white[1])
    y_min = np.min(id_white[0])  
    y_max = np.max(id_white[0])

    height = y_max - y_min
    width = x_max - x_min

    # Bounding box
    x, y, w, h = (x_min, y_min, width, height)
    cropped_img = img[y : y + h, x : x+w]
    
    padding = int(h * 0.25)

    # 5. Th√™m padding
    padded_img = cv.copyMakeBorder(
        cropped_img,
        padding,
        padding,
        padding,
        padding,
        cv.BORDER_CONSTANT,
        value=0,
    )

    # 6. Resize v·ªÅ 28x28
    if padded_img.shape[0] <= 0 or padded_img.shape[1] <= 0:
      st.warning("Kh√¥ng t√¨m th·∫•y h√¨nh v·∫Ω!")
      return np.zeros((28,28), dtype=np.uint8)

    final_img = cv.resize(padded_img, (28, 28), interpolation=cv.INTER_AREA)
    return final_img



def Applications():
    undo_symbol = "‚Ü©Ô∏è"
    trash_symbol = "üóëÔ∏è"
    st.markdown('<span style = "color:blue; font-size:24px;">C√°ch s·ª≠ d·ª•ng</span>', unsafe_allow_html=True)
    st.write("  - V·∫Ω ch·ªØ s·ªë c·∫ßn d·ª± ƒëo√°n **(0, 1, ...9)** l√™n h√¨nh ch·ªØ nh·∫≠t m√†u ƒëen ·ªü d∆∞·ªõi")
    st.write(f"  - Khi c·∫ßn ho√†n t√°c thao t√°c v·ª´a th·ª±c hi·ªán, **Click** chu·ªôt v√†o {undo_symbol} ·ªü d∆∞·ªõi ·∫£nh")
    st.write(f"  - Khi c·∫ßn Reset l·∫°i t·ª´ ƒë·∫ßu c√°c thao t√°c, **Click** chu·ªôt v√†o {trash_symbol} ·ªü d∆∞·ªõi ·∫£nh")
    st.write("  - Sau ƒë√≥ nh·∫•n n√∫t **Submit** ·ªü b√™n d∆∞·ªõi ƒë·ªÉ nh·∫≠n k·∫øt qu·∫£ d·ª± ƒëo√°n")
    st.write("**L∆∞u √Ω:** Ch·ªâ v·∫Ω m·ªôt ch·ªØ s·ªë **duy nh·∫•t**")
    stroke_width = 3
    stroke_color = "red"
    drawing_mode = "freedraw"
    image = Image.new("RGB", (280, 280), "black")
    c = st.columns([3, 7])
    with c[0]:
        canvas_result = st_canvas(
            fill_color = "rgba(255, 165, 0, 0.3)",
            stroke_width=stroke_width,
            stroke_color = stroke_color,
            background_color="#000000",
            background_image=image,
            # update_streamlit=realtime_update,
            width = image.width,
            height = image.height,
            drawing_mode=drawing_mode,
            key="Handwriting Letter Recognize",
        )
        c[1].markdown("**K·∫øt qu·∫£ d·ª± ƒëo√°n**")
        if st.button("Submit"):
            if canvas_result.image_data is not None:
                image_canvas = canvas_result.image_data
                if image_canvas.dtype != np.uint8:
                    image_canvas = cv.normalize(image_canvas, None, 0, 255, cv.NORM_MINMAX, dtype=cv.CV_8U)
                image_canvas = cv.cvtColor(image_canvas, cv.COLOR_BGR2GRAY)
                image_cpy = image_canvas.copy()
                image_canvas[image_cpy != 0] = 255
                # st.image(image_canvas)
                image_crop = crop_and_center_mnist(image_canvas)
                # st.image(image_crop)
                results = predict_with_image(image_crop)
                c[1].markdown(f"<p style='font-size: 30px;'>{results[0]}</p>", unsafe_allow_html=True)
            else:
                st.warning("Vui l√≤ng v·∫Ω k√≠ t·ª± c·∫ßn d·ª± ƒëo√°n tr∆∞·ªõc khi **Submit!**")

def Results():
    c = st.columns(2)
    with c[0]:
        st.markdown("D∆∞·ªõi ƒë√¢y l√† bi·ªÉu ƒë·ªì bi·ªÉu di·ªÖn ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p hu·∫•n luy·ªán (**train**) v√† t·∫≠p x√°c th·ª±c (**validation**)")
        image_accuracy = cv.imread("./images/Handwriting_Letter_Recognize/train_val_acc.PNG")
        st.image(image_accuracy, caption="Training and Validation Acccuracy", channels="BGR")
        accuracy_test = 0.976
        st.markdown(f"ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p **test** l√†: **{accuracy_test}**")
    with c[1]:
        st.markdown("D∆∞·ªõi ƒë√¢y l√† bi·ªÉu ƒë·ªì bi·ªÉu di·ªÖn ƒë·ªô m·∫•t m√°t tr√™n t·∫≠p hu·∫•n luy·ªán (**train**) v√† t·∫≠p x√°c th·ª±c (**validation**)")
        image_loss = cv.imread("./images/Handwriting_Letter_Recognize/train_val_loss.PNG")
        st.image(image_loss, caption="Training and Validation Loss", channels="BGR")

def Training():
    st.markdown("#### 2.3 Qu√° tr√¨nh hu·∫•n luy·ªán")
    st.markdown(
                """
                - Trong dataset **MNIST:**
                    - Chia t·∫≠p **training set** th√†nh 2 t·∫≠p:
                        - $50000$ ·∫£nh cho t·∫≠p hu·∫•n luy·ªán (train)
                        - $10000$ ·∫£nh cho t·∫≠p x√°c th·ª±c (validation)
                    - S·ª≠ d·ª•ng $10000$ ·∫£nh t·∫≠p **test set** l√†m t·∫≠p test.
                - C√°c tham s·ªë s·ª≠ d·ª•ng trong qu√° tr√¨nh hu·∫•n luy·ªán:
                    - H√†m t·ªëi ∆∞u: **Adam**
                    - H√†m m·∫•t m√°t: **sparse_categorical_crossentropy**
                    - ƒê·ªô ch√≠nh x√°c: **Accuracy**
                    - Learning_rate: **0.001**
                    - S·ªë l∆∞·ª£ng Epoch: **10**
                """
    )
def Text():
    st.header("1. Gi·ªõi thi·ªáu dataset MNIST")
    st.markdown(
                """
                - **MNIST (Modified National Institute of Standards and Technology)** l√† m·ªôt trong nh·ªØng b·ªô d·ªØ li·ªáu n·ªïi ti·∫øng v√† ph·ªï bi·∫øn nh·∫•t trong lƒ©nh v·ª±c h·ªçc m√°y, 
                ƒë·∫∑c bi·ªát l√† trong c√°c b√†i to√°n nh·∫≠n d·∫°ng h√¨nh ·∫£nh. **MNIST** ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ƒë√°nh gi√° c√°c thu·∫≠t to√°n ph√¢n lo·∫°i h√¨nh ·∫£nh, ƒë·∫∑c bi·ªát l√† c√°c m√¥ h√¨nh h·ªçc s√¢u **(deep learning)**
                """)
    c = st.columns(2)
    with c[0]:
        st.markdown("#### 1.1 T·ªïng quan v·ªÅ dataset MNIST")
        st.markdown(
                """
                - K√≠ch th∆∞·ªõc: Dataset **MNIST** g·ªìm c√≥ $70.000$ h√¨nh ·∫£nh, chia th√†nh hai ph·∫ßn:
                    - $60.000$ h√¨nh ·∫£nh hu·∫•n luy·ªán **(training set)**.
                    - $10.000$ h√¨nh ·∫£nh ki·ªÉm tra **(test set)**.
                """)
    with c[1]:
        st.markdown("#### 1.2 ƒê·∫∑c ƒëi·ªÉm dataset MNIST")
        st.markdown(
                """
                - K√≠ch th∆∞·ªõc h√¨nh ·∫£nh: M·ªói h√¨nh ·∫£nh c√≥ k√≠ch th∆∞·ªõc **28x28** pixel, v·ªõi m·ªói pixel l√† m·ªôt gi√° tr·ªã ƒë·ªô s√°ng **(grayscale)** t·ª´ $0$ ƒë·∫øn $255$.
                - ƒê·ªãnh d·∫°ng d·ªØ li·ªáu: D·ªØ li·ªáu c√≥ d·∫°ng **2D**, m·ªói ·∫£nh l√† m·ªôt ma tr·∫≠n **28x28**, v·ªõi m·ªói gi√° tr·ªã l√† ƒë·ªô s√°ng c·ªßa pixel (ƒë∆∞·ª£c chu·∫©n h√≥a t·ª´ $0$ ƒë·∫øn $1$ khi chia cho $255$).
                - K√Ω t·ª± vi·∫øt tay: C√°c h√¨nh ·∫£nh trong b·ªô d·ªØ li·ªáu ƒë∆∞·ª£c thu th·∫≠p t·ª´ c√°c c√¥ng d√¢n M·ªπ, bao g·ªìm c·∫£ tr·∫ª em v√† ng∆∞·ªùi tr∆∞·ªüng th√†nh. C√°c ch·ªØ s·ªë ƒë∆∞·ª£c vi·∫øt tay trong nhi·ªÅu phong c√°ch kh√°c nhau, 
                gi√∫p m√¥ h√¨nh h·ªçc m√°y ph·∫£i c√≥ kh·∫£ nƒÉng nh·∫≠n di·ªán ch·ªØ s·ªë vi·∫øt tay trong c√°c t√¨nh hu·ªëng ƒëa d·∫°ng.
                - L·ªõp nh√£n (Labels): M·ªói h√¨nh ·∫£nh c√≥ m·ªôt nh√£n t∆∞∆°ng ·ª©ng (label) l√† m·ªôt trong c√°c ch·ªØ s·ªë t·ª´ $0$ ƒë·∫øn $9$.
                """)
    st.markdown("D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë ·∫£nh minh h·ªça dataset **MNIST**")
    c = st.columns([2, 6, 2])
    image = cv.imread("./images/Handwriting_Letter_Recognize/dataset_MNIST.png")
    c[1].image(image, channels="BGR", caption="Minh h·ªça dataset MNIST")
    st.header("2 Ph∆∞∆°ng ph√°p")
    st.markdown(
            """
            - M√¥ h√¨nh ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ hu·∫•n luy·ªán c√°c k√≠ t·ª± trong dataset **MNIST** l√† m·ªôt m√¥ h√¨nh CNN g·ªìm $7$ l·ªõp bao g·ªìm nhi·ªÅu l·ªõp **convolutional (conv)** v√† **pooling**, 
            v·ªõi c√°c l·ªõp **fully connected (FC)** ·ªü cu·ªëi ƒë·ªÉ th·ª±c hi·ªán ph√¢n lo·∫°i.  
            """)
    st.markdown("#### 2.1 Ki·∫øn tr√∫c c·ªßa m√¥ h√¨nh")
    image_achitecture = cv.imread("./images/Handwriting_Letter_Recognize/Achitecture_model.PNG")
    cc = st.columns([3, 5, 2])
    cc[1].image(image_achitecture, caption="Ki·∫øn tr√∫c c·ªßa m√¥ h√¨nh", channels="BGR")
    st.markdown("##### 2.1.1 Feature extraction")
    st.markdown("**Feature extraction (tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng)** bao g·ªìm nhi·ªÅu l·ªõp **Convolutional(Conv)** v√† **Max Pooling**")
    
    c = st.columns(2)
    with c[0]:
        st.markdown("**2.1.1.1 L·ªõp Convolutional (Conv)**")
        st.markdown(
                    """
                    - **L·ªõp Convolutional (Conv)** l√† m·ªôt th√†nh ph·∫ßn quan tr·ªçng trong m·∫°ng n∆°-ron t√≠ch ch·∫≠p **(CNN)**. Ch·ª©c nƒÉng ch√≠nh c·ªßa n√≥ l√† tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng t·ª´ d·ªØ li·ªáu ƒë·∫ßu v√†o (·∫£nh ho·∫∑c t√≠n hi·ªáu)
                    th√¥ng qua **ph√©p t√≠ch ch·∫≠p** gi·ªØa c√°c b·ªô l·ªçc **(filters)** v√† d·ªØ li·ªáu.
                    """
        )
        image_conv = cv.imread("./images/Handwriting_Letter_Recognize/convolutional_layer.png")
        st.image(image_conv, caption="·∫¢nh minh ho·∫° ph√©p t√≠ch ch·∫≠p", channels="BGR")
    with c[1]:
        st.markdown("**2.1.1.2 L·ªõp Max pooling**")
        st.markdown(
                    """
                    - **L·ªõp Max Pooling** l√† m·ªôt k·ªπ thu·∫≠t hi·ªáu qu·∫£ ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc d·ªØ li·ªáu, tƒÉng t√≠nh b·∫•t bi·∫øn v·ªõi c√°c bi·∫øn ƒë·ªïi nh·ªè v√† tƒÉng c∆∞·ªùng tr∆∞·ªùng ti·∫øp nh·∫≠n trong **CNN**. N√≥ gi√∫p m√¥ h√¨nh h·ªçc c√°c ƒë·∫∑c tr∆∞ng
                     hi·ªáu qu·∫£ h∆°n, gi·∫£m chi ph√≠ t√≠nh to√°n v√† h·∫°n ch·∫ø **overfitting**. M·∫∑c d√π c√≥ m·ªôt s·ªë k·ªπ thu·∫≠t **pooling** kh√°c nh∆∞ **Average Pooling**, nh∆∞ng **Max Pooling** v·∫´n l√† ph∆∞∆°ng ph√°p ph·ªï bi·∫øn v√† th∆∞·ªùng cho k·∫øt qu·∫£ 
                     t·ªët h∆°n trong nhi·ªÅu b√†i to√°n th·ªã gi√°c m√°y t√≠nh.
                    """
        )
        image_maxpooling = cv.imread("./images/Handwriting_Letter_Recognize/MaxPooling_layer.png")
        st.image(image_maxpooling, caption="·∫¢nh minh ho·∫° ph√©p Max pooling", channels="BGR")
    st.markdown("D∆∞·ªõi ƒë√¢y l√† h√¨nh ·∫£nh minh ho·∫° qu√° tr√¨nh **tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (Feature extraction)** c·ªßa m√¥ h√¨nh")
    image_visualize = cv.imread("./images/Handwriting_Letter_Recognize/visualizeCNN.PNG")
    st.image(image_visualize, caption="H√¨nh ·∫£nh minh ho·∫° qu√° tr√¨nh tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng c·ªßa m√¥ h√¨nh", channels="BGR")
    st.markdown("##### 2.1.1 Classification")
    st.markdown(
                """
                - Sau khi tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng **(feature extraction)** t·ª´ c√°c l·ªõp **Convolutional(Conv)** v√† **Max pooling**, ch·ª©c nƒÉng c·ªßa ph·∫ßn **Classification** trong m√¥ h√¨nh h·ªçc s√¢u l√† s·ª≠ d·ª•ng nh·ªØng ƒë·∫∑c tr∆∞ng n√†y ƒë·ªÉ ph√¢n lo·∫°i 
                c√°c ƒë·ªëi t∆∞·ª£ng ho·∫∑c m·∫´u ƒë·∫ßu v√†o v√†o c√°c l·ªõp c·ª• th·ªÉ. Ph·∫ßn n√†y th·ª±c hi·ªán th√¥ng qua c√°c l·ªõp **Fully Connected (Dense)**, n∆°i c√°c ƒë·∫∑c tr∆∞ng tr√≠ch xu·∫•t ƒë∆∞·ª£c ƒë∆∞a v√†o v√† th√¥ng qua 
                qu√° tr√¨nh h·ªçc, ch√∫ng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒë∆∞a ra k·∫øt qu·∫£ ph√¢n lo·∫°i.
                """
    )
    st.markdown("D∆∞·ªõi ƒë√¢y l√† h√¨nh ·∫£nh minh ho·∫° l·ªõp **Fully Connected**")
    c = st.columns([3, 5, 2])
    image_FC = cv.imread("./images/Handwriting_Letter_Recognize/FullyConnected.PNG")
    c[1].image(image_FC, caption="Fully Connected", channels="BGR")
    st.markdown("#### 2.2 L√Ω do ch·ªçn ki·∫øn tr√∫c n√†y")
    st.markdown(
                """
                - Ki·∫øn tr√∫c n√†y ƒë∆∞·ª£c l·ª±a ch·ªçn v√¨ t√≠nh hi·ªáu qu·∫£ trong vi·ªác tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ d·ªØ li·ªáu ƒë·∫ßu v√†o, gi·∫£m s·ªë l∆∞·ª£ng tham s·ªë, 
                v√† kh·∫£ nƒÉng h·ªçc c√°c ƒë·∫∑c tr∆∞ng c·∫ßn thi·∫øt cho b√†i to√°n ph√¢n lo·∫°i ch·ªØ s·ªë. ƒê√¢y l√† m·ªôt c·∫•u tr√∫c c∆° b·∫£n nh∆∞ng m·∫°nh m·∫Ω, th∆∞·ªùng ƒë∆∞·ª£c 
                s·ª≠ d·ª•ng nh∆∞ b∆∞·ªõc kh·ªüi ƒë·∫ßu trong c√°c b√†i to√°n li√™n quan ƒë·∫øn ph√¢n lo·∫°i ·∫£nh nh·ªè.
                """
    )
    c = st.columns(2)
    with c[0]:
        st.markdown("##### 2.2.1 S·ª≠ d·ª•ng c√°c kh·ªëi **Convolutional**")
        st.markdown(
                    """
                    - L·ªõp **Conv2D:**
                        - **Vai tr√≤**: Tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng t·ª´ h√¨nh ·∫£nh ƒë·∫ßu v√†o, nh∆∞ c·∫°nh, g√≥c, v√† c√°c chi ti·∫øt c·∫•u tr√∫c c∆° b·∫£n.
                        - **B·ªô l·ªçc**: S·ªë l∆∞·ª£ng b·ªô l·ªçc tƒÉng d·∫ßn t·ª´ $32$ ƒë·∫øn $64$. ƒêi·ªÅu n√†y gi√∫p m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c nhi·ªÅu ƒë·∫∑c tr∆∞ng ph·ª©c t·∫°p h∆°n khi ƒëi s√¢u v√†o m·∫°ng.
                        - **K√≠ch th∆∞·ªõc b·ªô l·ªçc (3, 3)**: M·ªôt l·ª±a ch·ªçn ph·ªï bi·∫øn ƒë·ªÉ n·∫Øm b·∫Øt c√°c chi ti·∫øt nh·ªè trong ·∫£nh.
                        - **padding='same'**: ƒê·∫£m b·∫£o ƒë·∫ßu ra c·ªßa l·ªõp **Conv2D** c√≥ c√πng k√≠ch th∆∞·ªõc kh√¥ng gian v·ªõi ƒë·∫ßu v√†o, gi·ªØ th√¥ng tin ·ªü r√¨a ·∫£nh.
                    - Hai l·ªõp **Conv2D** li√™n ti·∫øp trong kh·ªëi:
                        - Vi·ªác x·∫øp ch·ªìng hai l·ªõp **Conv2D** gi√∫p m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c c√°c ƒë·∫∑c tr∆∞ng s√¢u v√† ph·ª©c t·∫°p h∆°n, thay v√¨ ch·ªâ h·ªçc c√°c ƒë·∫∑c tr∆∞ng c∆° b·∫£n t·ª´ m·ªôt l·ªõp duy nh·∫•t (ph√π h·ª£p v·ªõi anh x√°m c√≥ k√≠ch th∆∞·ªõc nh·ªè **(28x28)** nh∆∞ **MNIST**).
                    """
        )
        st.markdown("##### 2.2.2 Gi·∫£m k√≠ch th∆∞·ªõc kh√¥ng gian b·∫±ng **MaxPooling**")
        st.markdown(
                    """
                    - L·ªõp **MaxPooling2D**:
                        - **Vai tr√≤**: Gi·∫£m k√≠ch th∆∞·ªõc kh√¥ng gian (chi·ªÅu r·ªông v√† chi·ªÅu cao) c·ªßa c√°c ƒë·∫∑c tr∆∞ng, gi√∫p gi·∫£m s·ªë l∆∞·ª£ng tham s·ªë v√† tƒÉng t·ªëc ƒë·ªô t√≠nh to√°n.
                        - K√≠ch th∆∞·ªõc v√πng **pooling (2, 2)** l√† m·ªôt l·ª±a ch·ªçn ph·ªï bi·∫øn ƒë·ªÉ gi·∫£m m·ªôt n·ª≠a k√≠ch th∆∞·ªõc kh√¥ng gian.
                        - Lo·∫°i b·ªè c√°c th√¥ng tin kh√¥ng quan tr·ªçng, gi·ªØ l·∫°i c√°c ƒë·∫∑c tr∆∞ng n·ªïi b·∫≠t nh·∫•t trong m·ªói v√πng.
                    """
        )
    with c[1]:
        st.markdown("##### 2.2.3 Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh vector ph·∫≥ng")
        st.markdown(
                    """
                    - L·ªõp **Flatten()**:
                        - **Vai tr√≤**: Chuy·ªÉn ƒë·ªïi **tensor 3D** (k√≠ch th∆∞·ªõc kh√¥ng gian v√† s·ªë l∆∞·ª£ng b·ªô l·ªçc) th√†nh **vector 1D** ƒë·ªÉ l√†m ƒë·∫ßu v√†o cho c√°c l·ªõp **fully connected**.
                    """
        )
        st.markdown("##### 2.2.4  S·ª≠ d·ª•ng c√°c l·ªõp **Fully Connected** ƒë·ªÉ ph√¢n lo·∫°i")
        st.markdown(
                    """
                    - L·ªõp **Dense(128, activation='relu')**:
                        - **Vai tr√≤**: H·ªçc c√°c m·ªëi quan h·ªá phi tuy·∫øn t√≠nh gi·ªØa c√°c ƒë·∫∑c tr∆∞ng ƒë√£ tr√≠ch xu·∫•t v√† √°nh x·∫° ch√∫ng t·ªõi c√°c l·ªõp ph√¢n lo·∫°i.
                        - S·ªë l∆∞·ª£ng ƒë∆°n v·ªã ·∫©n l√† $128$, ƒë·ªß l·ªõn ƒë·ªÉ h·ªçc ƒë∆∞·ª£c c√°c ƒë·∫∑c tr∆∞ng ph·ª©c t·∫°p m√† v·∫´n gi·ªØ t√≠nh hi·ªáu qu·∫£ t√≠nh to√°n.
                    - L·ªõp **Dense(10, activation='softmax')**:
                        - **Vai tr√≤**: L·ªõp ƒë·∫ßu ra v·ªõi $10$ ƒë∆°n v·ªã t∆∞∆°ng ·ª©ng v·ªõi $10$ l·ªõp.
                        - H√†m k√≠ch ho·∫°t **softmax**: Chuy·ªÉn ƒë·ªïi c√°c gi√° tr·ªã th√†nh x√°c su·∫•t cho t·ª´ng l·ªõp, gi√∫p m√¥ h√¨nh d·ªÖ d√†ng ph√¢n lo·∫°i.
                    """
        )
    Training()
    st.markdown("#### 3. K·∫øt qu·∫£")
    Results()
    st.markdown("#### 4. ·ª®ng d·ª•ng")
    Applications()

def App():
    Text()
App()